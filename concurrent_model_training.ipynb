{"cells":[{"cell_type":"code","source":["# Install the fbprophet library, if required.\n%pip install fbprophet"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3bc07ee-06d5-43ca-a5be-caafba841116"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Required packages for this lab\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nimport time\nfrom fbprophet import Prophet \n\n# Suppress uninformative warnings and errors\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogger = spark._jvm.org.apache.log4j\nlogging.getLogger(\"py4j\").setLevel(logging.ERROR)\nlogging.getLogger('fbprophet').setLevel(logging.WARNING) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ba0b7b4-238a-4123-89a7-d8cf2324fa39"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Import dataset from csv file\ndf = pd.read_csv('/dbfs/FileStore/ConcurrentModelTraining/data.csv', parse_dates=['date'], header='infer')\ndf.head(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"774e4fc4-0b90-4b39-b74d-bbe0f9bff79c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Depending on cluster configuration\n# mode = 'single node'\nmode = 'four workers'\nnumber_disks = len(df.disk_id.unique())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14abfa92-12d3-4501-a3b1-87ead1e7f9e4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create Spark DataFrame for storing results\nfrom pyspark.sql.types import *\n\nschema_log_time = StructType([\n    StructField(\"method\",StringType(),True),\n    StructField(\"algorithm\",StringType(),True),\n    StructField(\"mode\",StringType(),True),\n    StructField(\"number_disks\",IntegerType(),True),\n    StructField(\"duration\",FloatType(),True),\n])\n\ndf_log_time = spark.createDataFrame(sc.emptyRDD(), schema_log_time)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60c6ce68-ed0c-44ae-b82d-ff7beeabb234"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# FBProphet algorithm for time series forecasting\n# https://facebook.github.io/prophet/docs/quick_start.html#python-api\ndef prophet_predict(df: pd.DataFrame) -> list:\n    df.reset_index(drop=True,inplace=True)\n    \n    # Split data into train test datasets\n    df_train = df[0:int(len(df)*0.8)]\n    df_test = df[int(len(df)*0.8):]\n    \n    # Number of periods to predict\n    steps = len(df_test)\n    \n    # Format the data for fbprophet \n    prophet_df = df_train[['date', 'free_space_gb']].copy()\n    prophet_df.columns = ['ds','y']\n    \n    # Train and fit the model\n    model = Prophet(\n          interval_width = 0.95,\n          growth = 'linear')\n    model.fit(prophet_df)\n\n    # Make the future prediction\n    future_pd = model.make_future_dataframe(\n      periods = steps,\n      freq = 'd',\n      include_history = False\n      )\n    prediction = model.predict(future_pd)\n    y_pred= prediction.yhat\n    \n    # Add column to DataFrame \n    df_full = df.copy()\n    df_full['prediction'] = [None] * df_test.index.min() + list(y_pred)\n    \n    # Calculate RMSE metric\n    rmse = np.sqrt(metrics.mean_squared_error(df_test.free_space_gb, y_pred))\n    \n    result = [df_full, rmse]\n    return result\n\n\n# Create our Linear Regression function\ndef regression(df:pd.DataFrame) -> list:\n    df.reset_index(drop=True,inplace=True)\n    X = df.index.values.reshape(-1,1)\n    y = df.iloc[:, 1].values\n    \n    # Split data into train test datasets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0,shuffle=False)\n\n    # Train and fit the model\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train)\n\n    # Make the future prediction\n    y_pred = regressor.predict(X_test)\n    date_pred = df[df.index>=X_test.min()].date\n\n    # Add column to DataFrame \n    df_full = df.copy()\n    df_full['prediction'] = [None] * X_test.min() + list(y_pred)\n    \n    # Calculate RMSE metric\n    rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n    result = [df_full, rmse]\n    return result"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a2853c0-1de4-41ff-86fe-070d96c4a051"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Sequential predictions fbprophet\nstart_time = time.perf_counter()\n\n# Create output DataFrames\ndf_fbprophet_sequential_prediction = pd.DataFrame(columns = ['date','free_space_gb','disk_id','prediction'])\ndf_fbprophet_sequential_rmse = pd.DataFrame(columns = ['disk_id','rmse'])\n\nfor c in df['disk_id'].unique():\n    df_each = df.loc[df['disk_id'] == c]\n    df_each_prediction, rmse = prophet_predict(df_each)\n    df_fbprophet_sequential_prediction = df_fbprophet_sequential_prediction.append(df_each_prediction, ignore_index=True)\n    df_fbprophet_sequential_rmse = df_fbprophet_sequential_rmse.append({'disk_id': c,'rmse':rmse}, ignore_index=True)\nduration_fbprophet_sequential = time.perf_counter() - start_time\nprint(\"--- {} seconds ---\".format(duration_fbprophet_sequential))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76a8c346-d091-4b6f-bebb-60b987f371cc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Save results\nrows = [['sequential', 'fbprophet', mode, number_disks, duration_fbprophet_sequential]]\ncolumns = ['method', 'algorithm', 'mode','number_disks', 'duration']\ndf_sequential_time = spark.createDataFrame(rows, columns)\n \n# Add this to our DataFrame\ndf_log_time = df_log_time.union(df_sequential_time)="],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40f28a91-04aa-41c4-94e3-ae7e41907a12"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Sequential predictions linear regression\nstart_time = time.perf_counter()\n\n# Create output DataFrames\ndf_regression_sequential_prediction = pd.DataFrame(columns = ['date','free_space_gb','disk_id','prediction'])\ndf_regression_sequential_rmse = pd.DataFrame(columns = ['disk_id','rmse'])\n\nfor c in df['disk_id'].unique():\n    df_each = df.loc[df['disk_id'] == c]\n    df_each_prediction, rmse = regression(df_each)\n    df_regression_sequential_prediction = df_regression_sequential_prediction.append(df_each_prediction, ignore_index=True)\n    df_regression_sequential_rmse = df_regression_sequential_rmse.append({'disk_id': c,'rmse':rmse}, ignore_index=True)\nduration_regression_sequential = time.perf_counter() - start_time\nprint(\"--- {} seconds ---\".format(duration_regression_sequential))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b38abb71-456b-4c6a-9891-579a5bb74ab6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Save results\nrows = [['sequential', 'linear regression', mode, number_disks, duration_regression_sequential]]\ncolumns = ['method', 'algorithm', 'mode','number_disks', 'duration']\ndf_sequential_time = spark.createDataFrame(rows, columns)\n \n# Add this to our DataFrame\ndf_log_time = df_log_time.union(df_sequential_time)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"974e74ec-c139-463c-b636-857ef02c1845"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Parallel processing with concurrent.futures for fbprophet\n# CPU intensive process, so will look to use ProcessPoolExecutor \nimport concurrent.futures\n\n# Create DataFrame of DataFrames to input into concurrent futures\ndisk_dfs = []\ndisks = df['disk_id'].unique()\nfor i in disks:\n    disk_dfs.append(df[df.disk_id == i].copy())\n\n# Create output DataFrames\ndf_fbprophet_concurrent_prediction = pd.DataFrame(columns = ['date','free_space_gb','disk_id','prediction'])\ndf_fbprophet_concurrent_rmse = pd.DataFrame(columns = ['disk_id','rmse'])\n\nstart_time = time.perf_counter()\n\n# Concurrent futures method of processing predictions in parallel fashion\nwith concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n    future = list(map(lambda x: executor.submit(prophet_predict, x), disk_dfs))\n    finished,unfinished = concurrent.futures.wait(future)\n    for x in finished:\n        try:\n            df_fbprophet_concurrent_prediction = df_fbprophet_concurrent_prediction.append(x.result()[0])\n            disk_id = x.result()[0].disk_id[0]\n            rmse = x.result()[1]\n            new_row = {'disk_id': disk_id,'rmse':rmse}\n            df_fbprophet_concurrent_rmse = df_fbprophet_concurrent_rmse.append(new_row, ignore_index=True)\n        except Exception as e:\n            print(e,type(e))\n            \nduration_fbprophet_concurrent = time.perf_counter() - start_time\nprint(\"--- {} seconds ---\".format(duration_fbprophet_concurrent))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e85cc32-473a-4ca9-bea9-626d88be31ce"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Save results\nrows = [['concurrent.futures', 'fbprophet', mode, number_disks, duration_fbprophet_concurrent]]\ncolumns = ['method', 'algorithm', 'mode','number_disks', 'duration']\ndf_concurrent_time = spark.createDataFrame(rows, columns)\n\n# Add this to our DataFrame\ndf_log_time = df_log_time.union(df_concurrent_time)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3fbf4df-3f62-4a2c-9b3e-8c7f457288b4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Parallel processing with concurrent.futures for linear regression\n# CPU intensive process, so will look to use ProcessPoolExecutor \nimport concurrent.futures\n\n# Create DataFrame of DataFrames to input into concurrent futures\ndisk_dfs = []\ndisks = df['disk_id'].unique()\nfor i in disks:\n    disk_dfs.append(df[df.disk_id == i].copy())\n\n# Create output DataFrames\ndf_regression_concurrent_prediction = pd.DataFrame(columns = ['date','free_space_gb','disk_id','prediction'])\ndf_regression_concurrent_rmse = pd.DataFrame(columns = ['disk_id','rmse'])\n\nstart_time = time.perf_counter()\n\n# Concurrent futures method of processing predictions in parallel fashion\nwith concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n    future = list(map(lambda x: executor.submit(regression, x), disk_dfs))\n    finished,unfinished = concurrent.futures.wait(future)\n    for x in finished:\n        try:\n            df_regression_concurrent_prediction=df_regression_concurrent_prediction.append(x.result()[0])\n            disk_id = x.result()[0].disk_id[0]\n            rmse = x.result()[1]\n            new_row = {'disk_id': disk_id,'rmse':rmse}\n            df_regression_concurrent_rmse = df_regression_concurrent_rmse.append(new_row, ignore_index=True)\n        except Exception as e:\n            print(e,type(e))\n            \nduration_regression_concurrent = time.perf_counter() - start_time\nprint(\"--- {} seconds ---\".format(duration_regression_concurrent))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b29a084-25bd-4215-8855-4f403467eb02"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Save results\nrows = [['concurrent.futures', 'linear regression', mode, number_disks, duration_regression_concurrent]]\ncolumns = ['method', 'algorithm', 'mode','number_disks', 'duration']\ndf_concurrent_time = spark.createDataFrame(rows, columns)\n\n# Add this to our DataFrame\ndf_log_time = df_log_time.union(df_concurrent_time)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5837d20-e303-464a-b082-73edfdd1e575"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\n\n# Define schema of output results\ninput_schema =StructType([\n  StructField('date',DateType()),\n  StructField('free_space_gb',FloatType()),\n  StructField('disk_id',IntegerType())\n])\n\n#Create Spark DataFrame of csv file\nspark_df = spark.read \\\n    .option(\"header\", True) \\\n    .schema(input_schema) \\\n    .csv(\"dbfs:/FileStore/ConcurrentModelTraining/data.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1df00af4-411d-4fbd-b00a-8d5140458eea"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Define schema of output results\nresult_schema =StructType([\n  StructField('date',DateType()),\n  StructField('free_space_gb',FloatType()),\n  StructField('disk_id',IntegerType()),\n  StructField('prediction',FloatType())\n])\n\n# Define schema of evaluation results\nevaluation_schema =StructType([\n  StructField('disk_id',IntegerType()),\n  StructField('rmse',FloatType())\n])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cbd8e73e-406c-4bd7-b59c-1d240b93ed58"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create new fbprophet function for Pandas UDFs\n# No longer need to specify the @pandas_udf(df.schema, PandasUDFType.GROUPED_MAP) decorator since Apache Spark 3.0:\n# https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html\ndef prophet_predict_udf(df: pd.DataFrame) -> pd.DataFrame:\n    df.reset_index(drop=True,inplace=True)\n    \n    # Split data into train test datasets\n    df_train = df[0:int(len(df)*0.8)]\n    df_test = df[int(len(df)*0.8):]\n    \n    # Number of periods to predict\n    steps = len(df_test)\n\n    # Format the data for fbprophet\n    prophet_df = df_train[['date', 'free_space_gb']].copy()\n    prophet_df.columns = ['ds','y']\n    \n    # Train and fit the model\n    model = Prophet(\n          interval_width = 0.95,\n          growth = 'linear')\n    model.fit(prophet_df)\n\n    # Make the future predictio\n    future_pd = model.make_future_dataframe(\n      periods = steps,\n      freq = 'd',\n      include_history = False\n      )\n\n    prediction = model.predict(future_pd)\n    y_pred= prediction.yhat\n    \n    # Add column to DataFrame \n    df_full = df.copy()\n    df_full['prediction'] = [None] * df_test.index.min() + list(y_pred)\n    \n    return df_full\n\n# Create new regression function for Pandas UDFs\ndef regression_udf(df: pd.DataFrame) -> pd.DataFrame:\n    df.reset_index(drop=True,inplace=True)\n    X = df.index.values.reshape(-1,1)\n    y = df.iloc[:, 1].values\n    \n    # Split data into train test datasets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0,shuffle=False)\n    \n    # Train and fit the model\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train)\n    \n    # Make the future prediction\n    y_pred = regressor.predict(X_test)\n    date_pred = df[df.index>=X_test.min()].date\n    \n    # Add column to DataFrame\n    df_full = df.copy()\n    df_full['prediction'] = [None] * X_test.min() + list(y_pred)\n\n    return df_full\n\n\n# Separate out RMSE function \ndef evaluation_rmse(df: pd.DataFrame) -> pd.DataFrame:\n    disk = df.disk_id.iloc[0]\n    \n    # Calculate RMSE metric\n    rmse = np.sqrt(metrics.mean_squared_error(df.dropna().free_space_gb, df.dropna().prediction))\n\n    results = {'disk_id':[disk], 'rmse':[rmse]}\n    return pd.DataFrame.from_dict( results )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0465584b-ebe4-45b0-898d-13e4aa681fc4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Parallel executions with Pandas UDFs for fbprophet\nstart_time = time.perf_counter()\n\n# Create output DataFrames\ndf_fbprophet_udfs_prediction = spark_df.groupBy('disk_id').applyInPandas(prophet_predict_udf, result_schema).cache()\ndf_fbprophet_udfs_rmse = df_fbprophet_udfs_prediction.groupBy('disk_id').applyInPandas(evaluation_rmse, evaluation_schema).cache()\n\n# df_fbprophet_udfs_prediction.show()\ndf_fbprophet_udfs_rmse.show()\n\nduration_fbprophet_udfs = time.perf_counter() - start_time\nprint(\"--- {} seconds ---\".format(duration_fbprophet_udfs))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82c1545a-9a24-4142-8965-48107a8bef64"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Save results\nrows = [['pandas UDFs', 'fbprophet', mode, number_disks, duration_fbprophet_udfs]]\ncolumns = ['method', 'algorithm', 'mode','number_disks', 'duration']\ndf_udfs_time = spark.createDataFrame(rows, columns)\n\n# Add this to our DataFrame\ndf_log_time = df_log_time.union(df_udfs_time)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bf3dd33-fe7f-40a8-9845-962bd00de6ab"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Parallel executions with Pandas UDFs for linear regression\nstart_time = time.perf_counter()\n\n# Create output DataFrames\ndf_regression_udfs_prediction = spark_df.groupBy('disk_id').applyInPandas(regression_udf, result_schema).cache()\ndf_regression_udfs_rmse = df_regression_udfs_prediction.groupBy('disk_id').applyInPandas(evaluation_rmse, evaluation_schema).cache()\n\n# df_regression_udfs_prediction.show()\ndf_regression_udfs_rmse.show()\n\nduration_regression_udfs = time.perf_counter() - start_time\nprint(\"--- {} seconds ---\".format(duration_regression_udfs))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90bb328a-8711-4931-b03c-4b31bd488b67"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Save results\nrows = [['pandas UDFs', 'linear regression', mode, number_disks, duration_regression_udfs]]\ncolumns = ['method', 'algorithm', 'mode','number_disks', 'duration']\ndf_udfs_time = spark.createDataFrame(rows, columns)\n\n# Add this to our DataFrame\ndf_log_time = df_log_time.union(df_udfs_time)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ad8c875-75ef-460c-bfca-2e13fdf9be5a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Sequential predictions both algorithms\nstart_time = time.perf_counter()\n\n# Create output DataFrames\ndf_fbprophet_sequential_prediction = pd.DataFrame(columns = ['date','free_space_gb','disk_id','prediction'])\ndf_fbprophet_sequential_rmse = pd.DataFrame(columns = ['disk_id','rmse'])\ndf_regression_sequential_prediction = pd.DataFrame(columns = ['date','free_space_gb','disk_id','prediction'])\ndf_regression_sequential_rmse = pd.DataFrame(columns = ['disk_id','rmse'])\n\nfor c in df['disk_id'].unique():\n    df_each = df.loc[df['disk_id'] == c]\n    \n    # FBProphet predictions\n    df_each_fbprophet_prediction, fbprophet_rmse = prophet_predict(df_each)\n    df_fbprophet_sequential_prediction = df_fbprophet_sequential_prediction.append(df_each_fbprophet_prediction, ignore_index=True)\n    df_fbprophet_sequential_rmse = df_fbprophet_sequential_rmse.append({'disk_id': c,'rmse':fbprophet_rmse}, ignore_index=True)\n    \n    # Linear regression predictions\n    df_each_regression_prediction, regression_rmse = regression(df_each)\n    df_regression_sequential_prediction = df_regression_sequential_prediction.append(df_each_regression_prediction, ignore_index=True)\n    df_regression_sequential_rmse = df_regression_sequential_rmse.append({'disk_id': c,'rmse':regression_rmse}, ignore_index=True)\n    \n    # Combine RMSE DataFrames\n    df_rmse_sequential = pd.merge(df_fbprophet_sequential_rmse,df_regression_sequential_rmse,how=\"inner\",on='disk_id', suffixes=('_fbprophet','_regression'))    \n    \nduration_combined_sequential = time.perf_counter() - start_time\nprint(\"--- {} seconds ---\".format(duration_combined_sequential))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c6e17ca-6616-48dc-a23e-17e9200eaf92"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rows = [['sequential', 'combined', mode, number_disks, duration_combined_sequential]]\ncolumns = ['method', 'algorithm', 'mode','number_disks', 'duration']\ndf_sequential_combined_time = spark.createDataFrame(rows, columns)\n \n# Add this to our DataFrame\ndf_log_time = df_log_time.union(df_sequential_combined_time)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3569214b-65e0-4da3-8fd7-e0eca3df06d9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def concurrent_full(df:pd.DataFrame) -> pd.DataFrame:\n    # FBProphet predictions\n    df_fbprophet_prediction, fbprophet_rmse = prophet_predict(df)\n    \n    # Linear regression predictions\n    df_regression_prediction, regression_rmse = regression(df)\n    \n    # Create RMSE DataFrame\n    disk_id = df.disk_id[0]\n    df_rmse = pd.DataFrame({'disk_id': [disk_id], 'rmse_fbprophet':[fbprophet_rmse],'rmse_regression':[regression_rmse]})\n    \n    result = [df_fbprophet_prediction, df_regression_prediction, df_rmse]\n    return result"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11f62ed9-a415-4763-a322-827aebea0b84"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Parallel processing with concurrent.futures\n# CPU intensive process, so will look to use ProcessPoolExecutor \nimport concurrent.futures\n\n# Create DataFrame of DataFrames to input into concurrent futures\ndisk_dfs = []\ndisks = df['disk_id'].unique()\nfor i in disks:\n    disk_dfs.append(df[df.disk_id == i].copy())\n\n# Create output DataFrames\ndf_fbprophet_concurrent_prediction = pd.DataFrame(columns = ['date','free_space_gb','disk_id','prediction'])\ndf_regression_concurrent_prediction = pd.DataFrame(columns = ['date','free_space_gb','disk_id','prediction'])\ndf_rmse_concurrent = pd.DataFrame(columns = ['disk_id','rmse_fbprophet', 'rmse_regression'])\n\nstart_time = time.perf_counter()\n\n# Concurrent futures method of processing predictions in parallel fashion\nwith concurrent.futures.ProcessPoolExecutor(max_workers=8) as executor:\n    future = list(map(lambda x: executor.submit(concurrent_full, x), disk_dfs))\n    finished,unfinished = concurrent.futures.wait(future)\n    for x in finished:\n        try:\n            df_fbprophet_concurrent_prediction = df_fbprophet_concurrent_prediction.append(x.result()[0])\n            df_regression_concurrent_prediction = df_regression_concurrent_prediction.append(x.result()[1])\n            df_rmse_concurrent = df_rmse_concurrent.append(x.result()[2])\n        except Exception as e:\n            print(e,type(e))\n \n            \nduration_combined_concurrent = time.perf_counter() - start_time\nprint(\"--- {} seconds ---\".format(duration_combined_concurrent))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53416be4-bc07-4e35-b3b3-0f8ae5413d5e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Save results\nrows = [['concurrent.futures', 'combined', mode, number_disks, duration_combined_concurrent]]\ncolumns = ['method', 'algorithm', 'mode', 'number_disks', 'duration']\ndf_concurrent_combined_time = spark.createDataFrame(rows, columns)\n \n# Add this to our DataFrame\ndf_log_time = df_log_time.union(df_concurrent_combined_time)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a4dff7d-68f5-4b5e-874f-9b5fac128814"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Parallel executions with Pandas UDFs\nstart_time = time.perf_counter()\n\n# Create output DataFrames\ndf_fbprophet_udfs_prediction = spark_df.groupBy('disk_id').applyInPandas(prophet_predict_udf, result_schema).cache()\ndf_fbprophet_udfs_rmse = df_fbprophet_udfs_prediction.groupBy('disk_id').applyInPandas(evaluation_rmse, evaluation_schema).cache()\ndf_regression_udfs_prediction = spark_df.groupBy('disk_id').applyInPandas(regression_udf, result_schema).cache()\ndf_regression_udfs_rmse = df_regression_udfs_prediction.groupBy('disk_id').applyInPandas(evaluation_rmse, evaluation_schema).cache()\n\ndf_fbprophet_udfs_rmse = df_fbprophet_udfs_rmse.withColumnRenamed('rmse','rmse_fbprophet').withColumnRenamed('disk_id','disk_id_fbprophet')\ndf_regression_udfs_rmse = df_regression_udfs_rmse.withColumnRenamed('rmse','rmse_regression').withColumnRenamed('disk_id','disk_id_regression')\ndf_rmse_udfs = df_fbprophet_udfs_rmse.join(\n    df_regression_udfs_rmse,\n    df_fbprophet_udfs_rmse.disk_id_fbprophet == df_regression_udfs_rmse.disk_id_regression,\n    \"inner\")\ndf_rmse_udfs.select(\"disk_id_fbprophet\",\"rmse_fbprophet\",\"rmse_regression\").show()\n\nduration_combined_udfs = time.perf_counter() - start_time\nprint(\"--- {} seconds ---\".format(duration_combined_udfs))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4657769-73e0-4387-a1ee-83baaddf979c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rows = [['pandas UDFs', 'combined', mode, number_disks, duration_combined_udfs]]\ncolumns = ['method', 'algorithm', 'mode', 'number_disks', 'duration']\ndf_udfs_combined_time = spark.createDataFrame(rows, columns)\n\n# Add this to our DataFrame\ndf_log_time = df_log_time.union(df_udfs_combined_time)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"508dbb3a-bf0b-4564-a7de-a16d9073bf49"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_log_time.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71089157-8282-4eaa-9189-4b7e272dda44"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Final step: add to SQL table and create visuals.\ndf_log_time.write.mode(\"append\").saveAsTable(\"methods\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbc1a165-5520-4d0a-b01a-2c8bb67a572c"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"concurrent_model_training","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4079011471360008}},"nbformat":4,"nbformat_minor":0}
